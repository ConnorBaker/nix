Approaches to Memoization in the Nix Evaluator: Correctness, Laziness, and GC-Safe Caching

Introduction and Problem Overview

Nix is a lazy, pure functional language used for package management. Its evaluator employs call-by-need (lazy evaluation) using thunks to delay computation ￼ ￼. In practice, this means that if the same expensive expression (e.g. the Nixpkgs library) is evaluated multiple times in one run, Nix will repeat the work unless the user explicitly shares the value. The goal of memoization is to automatically cache and share the results of function calls so that identical calls (same function + same argument values) reuse prior results instead of recomputing. This is especially useful for Nix, for example, to avoid evaluating the entire Nixpkgs for two different but identical nixosSystem instantiations ￼ ￼.

However, implementing memoization in Nix’s C++ evaluator is challenging. We must achieve caching without breaking laziness or correctness. Key difficulties include:
	•	Lazy values (thunks) as arguments – we cannot force thunks just to generate a cache key, as that would defeat laziness and could introduce side effects ￼. We need a way to hash or identify unevaluated structures.
	•	Hashing and equality – even if we hash arguments structurally, we must handle possible hash collisions by checking equality in a lazy-safe way (again, without forcing thunks) ￼ ￼.
	•	Garbage Collection (GC) – Nix uses Boehm GC, which can reuse freed memory addresses for new values. Pointer addresses alone are not reliable identifiers ￼. We need GC-safe keys that won’t confuse distinct values across time.
	•	Pointer mutation and sharing – Nix thunks are updated in-place when evaluated (the thunk object is mutated to hold the result) ￼. This means a value’s type/structure can change over time, complicating hashing (a value hashed before forcing might look different after forcing) ￼.
	•	Function closures and currying – Functions in Nix carry an environment (closure). The same function code with different captured environments should not be confused. Also, memoizing partial applications (curried functions) can increase sharing but might retain large environments longer than expected ￼.

In the sections below, we explore each of these issues and outline strategies to overcome them. We draw on approaches from other lazy functional runtimes (Haskell GHC, OCaml, Scheme) and research on content-addressed caching. Finally, we propose an implementation sketch for adding memoization to the Nix evaluator, emphasizing correctness, GC-safety, and preservation of laziness throughout.

Structural Hashing of Lazy Arguments (No Forcing)

To memoize a function call, we need a hash key that represents the function’s arguments without evaluating them. A straightforward content hash (like serializing the value) would force thunks and defeat the purpose ￼. Instead, we use a structural hash that traverses the shape of the value and incorporates known data, while treating thunks as opaque.

Key requirement: If an argument or any part of it is a thunk (unevaluated), the hashing mechanism must not force it – it should either omit that part or use a placeholder so that the thunk remains lazy ￼.

A practical approach is to implement a function hashValueLazy(Value* v) that pattern-matches on the Nix value type and computes a hash as follows:
	•	Primitive scalars: For integers, booleans, floats, or null, the hash can incorporate the numeric value or a type-tagged version of it. These are fully evaluated by nature (no thunks) ￼.
	•	Strings with context: Nix strings carry an attached context (derivation or store path dependencies) which affects their meaning ￼. The hash must include both the string content and its context list. For example, we can mix the string’s hash with a hash of each context element (which are stored in a canonical sorted order for determinism) ￼ ￼. Two strings that differ in context (even if text is same) should produce different hashes ￼.
	•	Paths: Nix store paths (file system paths) can be treated similar to strings (hash the path string). If needed, we might also incorporate some identifier of the path’s content or origin, but typically the path string (which includes a hash in Nix store paths) is enough to distinguish it ￼.
	•	Lists: Include the list length and combine hashes of each element recursively ￼. This requires iterating through the list structure. If any element is a thunk and returns a special “lazy” hash (see below), propagate that up.
	•	Attribute sets (maps): Nix attrsets are essentially dictionaries with string keys. The structural hash can combine the number of attributes and each attribute name with the hash of its value ￼ ￼. We must be careful to account for Nix’s attrset semantics: sets can be layered (through the // override operator), and their iteration sees a merged view ￼. The hashing should operate on the effective final set of bindings (e.g. using the same order that equality would use – typically sorted by key) to ensure canonical results. Cycle detection is also important: an attrset can be recursive (self-referential), so we need to keep a visited set of pointers to detect loops and avoid infinite recursion ￼.
	•	Functions (Lambdas and built-in functions): Functions are identified by their code and environment. We cannot meaningfully hash a function by traversing its content (the environment may contain huge graphs). Instead, we treat function values as opaque and use an identity-based hash. For a lambda (user-defined function), a good choice is to combine the pointer to the function’s AST (the code) with the pointer (or a stable ID) of its environment ￼. This way, two distinct closures (same code with different captured variables) will hash differently, whereas two references to the same closure will hash the same. Primitive operations or external C++ functions can likewise be hashed by some unique identifier (e.g., a function pointer or an index in a function table) ￼.
	•	Thunks (lazy values) and Apps (function applications): These represent unevaluated expressions. We do not descend into thunks – instead, if hashValueLazy encounters a value that is still a tThunk or tApp, it should bail out. In practice, we can return a distinguished hash value (for example 0 or a special tag) to indicate “this value is not hashable without forcing” ￼ ￼. This signals the memoization mechanism to skip caching for calls with such arguments. By doing so, we preserve laziness: we won’t accidentally evaluate a thunk just to compute a hash. The downside is we miss caching opportunities for functions that take large lazy structures, but it’s the safe route. (We will discuss more advanced options below, like optimistic hashing of thunks by identity.)

GC-safe hashing: One naive idea for hashing thunks without forcing is to hash their identity (e.g., the pointer values of the Env* and Expr* that make up the thunk) ￼. Indeed, a thunk in Nix is defined by an environment and an expression pointer ￼. While (Env*, Expr*) might uniquely identify a thunk at creation time, it is not reliable over the long run because of GC pointer reuse. Boehm GC can free a thunk that’s no longer needed and later allocate a new thunk at the same memory address, potentially even with the same (Env*, Expr*) combination by coincidence ￼. This scenario was observed to cause incorrect cache hits (returning a result from an unrelated computation) when a simple pointer-hash was tried in experiments ￼. Therefore, raw pointer addresses must not be used as hash keys after GC – they are not stable.

Instead, to handle thunks or other heap-allocated structures, we consider a few GC-safe techniques:
	•	Allocation IDs: Augment the allocator to give each new thunk (or each Value) a unique ID (e.g., a 64-bit counter) that is never reused ￼. This ID serves as a stand-in for the pointer. Even if the object is freed, no future object will get the same ID, avoiding confusion. We could incorporate this ID into the hash whenever we would have used the pointer. (For example, hash a thunk by hashing its Expr* plus the thunk’s unique ID instead of the raw Env* address.)
	•	AST fingerprinting: Since the Expr* (pointer to AST node) is stable (the AST is typically long-lived or constant during evaluation), one could hash the structure of the expression itself. For instance, compute a fingerprint for the AST subtree of the thunk’s expression (and maybe combine with fingerprints of its environment variables). This is like content-addressing the code. It’s stable across GCs and even across runs if the code doesn’t change. The Nix AST, however, might not capture the values of captured variables – only their positions. We could combine an AST hash with identifiers for the environment (e.g., environment ID as above). Note: This can be expensive and tricky (environments themselves are graphs), and may consider two thunks equal that are not truly equivalent (if they close over different data).
	•	Weak references in hashing: If we did use pointers as part of hashes, we should at least not keep those pointers forever in a way that confuses GC. Using a weak map from pointer to some hash component can allow the entry to vanish once the object is collected. In practice, though, it’s simpler to avoid raw pointers in the key entirely or ensure the objects are anchored (see GC section below).

In summary, the hashing strategy is: hash as much of the structure as possible without forcing any lazy components. If a thunk is encountered, we conservatively give up on caching that call (or treat it specially). All hashes must account for the full semantics of values (including string contexts, attrset merging, etc.) so that if two argument structures are logically the same, they get the same hash ￼ ￼. At the same time, the hash must incorporate any differences that would lead to different results. And finally, anything that could vary non-deterministically (like memory address) should either be replaced with a stable surrogate or avoided.

Pseudocode – lazy-safe hash function:

size_t hashValueLazy(const Value* v) {
    std::unordered_set<const Value*> visited;
    std::function<size_t(const Value*)> hashRec = [&](const Value* x) -> size_t {
        if (!x) return 0; 
        if (visited.count(x)) return 0; // prevent infinite loops for cycles
        visited.insert(x);
        switch (x->type()) {
        case tInt:   return std::hash<long>()(x->integer());
        case tBool:  return std::hash<bool>()(x->boolean());
        case tFloat: return std::hash<double>()(x->floating());
        case tNull:  return 1469598103934665603ULL;  // FNV offset basis as example for null
        case tString: {
            // hash string content
            size_t h = std::hash<std::string_view>()(x->string_view());
            // include context if present
            if (auto *ctx = x->stringContext()) {
                hashCombine(h, ctx->size());              // combine length of context
                for (auto *elem : *ctx) {
                    hashCombine(h, std::hash<std::string_view>()(elem->view()));
                }
            }
            return h;
        }
        case tPath: {
            // treat like string
            return std::hash<std::string_view>()(x->path()); 
        }
        case tList: {
            size_t h = std::hash<size_t>()(x->listSize());
            for (auto* elem : x->listElements()) {
                hashCombine(h, hashRec(elem));
            }
            return h;
        }
        case tAttrs: {
            // iterate over sorted attributes
            size_t h = std::hash<size_t>()(x->attrSize());
            for (auto& [key, val] : x->attrBindings()) {
                hashCombine(h, std::hash<std::string>()(key));
                hashCombine(h, hashRec(val));
            }
            return h;
        }
        case tLambda: {
            // Hash by identity: code pointer + env pointer or env ID
            size_t h = hashPointer(x->lambdaCode());
            hashCombine(h, hashPointer(x->lambdaEnv()));
            return h;
        }
        case tPrimOp: {
            return hashPointer(x->primOpFun()); // function pointer or unique id for builtins
        }
        case tExternal: {
            return hashPointer(x->externalType()); // assume external carries type or id
        }
        case tThunk:
        case tApp:
            return 0;  // cannot hash unevaluated value lazily
        default:
            return 0;
        }
    };
    return hashRec(v);
}

In this pseudo-code, hashCombine is a helper to mix hashes (e.g., boost::hash_combine), and hashPointer would safely hash a pointer (e.g., reinterpret it to integer but in a stable way, or better, use an associated ID). We return 0 for any thunk or function application encountered, meaning “do not memoize” ￼. Also note cycle protection via the visited set for attrsets/lists that reference themselves ￼.

This function gives us a lazy-safe structural hash of an argument. It is conservative (might return 0 in many cases), but ensures we never force a value. Next, we’ll see how to handle cases where we do get a hash and use it in the memo cache, including verifying equality to avoid collisions.

Lazy-Safe Structural Equality (Hash Collision Defense)

Even a perfect structural hash can collide if two different values happen to produce the same hash number. Given we plan to use these hashes as cache keys, it’s critical to guard against collisions – otherwise we could return the wrong result for a given input, a severe correctness bug ￼. The solution is to perform a structural equality check between the new call’s arguments and the cached entry’s arguments whenever we get a hash match ￼ ￼.

Designing an equality function for Nix values that is lazy-safe has similar challenges to the hashing function:
	•	It must not force any thunks in either structure. If either side has an unevaluated component, we cannot conclude they’re equal (and we won’t try to evaluate to find out, because that would change program behavior). In such cases, we should return “not equal” (or an indeterminate result that causes a cache miss). This effectively means we only consider two values equal if they are fully evaluated (no thunks) or at least structurally comparable without evaluation.
	•	It must handle all value types and recursively traverse lists and attrsets, with cycle detection, just like hashing did.
	•	For strings, it should compare both the text and the context dependencies ￼.
	•	For numbers and booleans, direct comparison.
	•	For attrsets and lists, length and then element-by-element comparisons (keys and values for attrsets) ￼.
	•	For functions, one tricky point: do we consider two function values equal? In Nix’s semantics, functions are reference types – there’s no extensional equality on functions (you can’t easily test if two lambdas are the “same” mapping). However, for memoization, if we are calling the same function with the same args, it means the function pointer (code) and environment are the same. So we can treat two lambdas as equal if and only if they are the exact same closure (same ExprLambda* code pointer and same Env* pointer) ￼. This is essentially pointer equality on the closure. We won’t be caching calls across different function instances anyway (the cache key includes the function identity), so this primarily matters if the equality check is used to verify we got the right closure – which it will if we store the function identity as part of the key. For builtins or external functions, we can likewise treat them as equal if they are the same pointer/reference.
	•	No thunks: If either value in a comparison is a thunk or application, we should immediately consider them not equal (or return an “incomparable” state). We do not traverse into thunks ￼. This aligns with the hashing: if thunks were present, we wouldn’t have cached, and if we did (via an optimistic strategy), we rely on this equality check to catch any mismatches.

We can implement equalValueLazy(a, b) in a structure similar to the hash function:
	•	If a and b have different types (one is an Int, the other is a List, etc.), return false.
	•	If they are both numbers or strings, compare their contents (and contexts for strings) ￼.
	•	If both are attrsets, compare the set of keys (the names) first – if lengths differ or any key differs, not equal. Then recursively compare each value by calling equalValueLazy on them. If any pair is not equal, stop and return false.
	•	If both are lists, compare lengths then each element pair recursively.
	•	If both are lambdas (functions), compare their code pointer and environment pointer (or unique environment ID). If both match, we treat them as equal (essentially they are the same closure or identical closure created separately, which in Nix can happen if the same function literal is evaluated in the same environment twice – but typically each evaluation yields a new Value* for the lambda, so pointer equality is a reasonable proxy).
	•	Cycles: maintain a visited set of pointer pairs (a_ptr, b_ptr) to handle cycles. If we revisit a pair, we can consider them equal up to that point (prevents infinite loops in recursive structures, much like how equal? in Scheme might handle cyclic graphs by remembering visited pairs).
	•	If either side is a thunk (tThunk or tApp) and not already handled, we return false (or an std::optional<bool> with “indeterminate”). In practice, we can just return false to be safe – meaning if one structure wasn’t fully evaluated, we won’t consider it equal to another. This might be slightly over-conservative (two thunks that are actually the same unevaluated expression in memory could be considered equal, but if they are literally the same pointer, they’d likely be the same object and probably one side wouldn’t be thunk if other isn’t – so this is a niche scenario).

This equality check is used only after a potential cache hit is found via the hash. So performance is less critical than the hash (which runs on every call to decide the key). Equality will run only when there is a hash match, which should be relatively rare (mostly when it is truly the same value, or a hash collision scenario).

Using equality to defend collisions: The memoization lookup algorithm will be:
	1.	Compute argsHash = hashValueLazy(arg). If this indicates a thunk (special value, e.g. 0), then skip caching for this call entirely ￼.
	2.	Use a composite key (functionIdentity, argsHash) to look up in the cache (functionIdentity could be the ExprLambda* and Env* for the function being called ￼).
	3.	If no entry is found, proceed to evaluate the function normally (cache miss).
	4.	If an entry is found, it means we have an earlier result with the same hash. We then retrieve the stored argument (or a representation of it) from the cache entry. Call equalValueLazy(currentArg, cachedArg). If this returns true, then we have confirmed that the current call’s argument is structurally equal to the original cached argument, so we can safely reuse the cached result ￼. If it returns false (or cannot confirm equality), then we consider it a hash collision or mismatch and do not use the cached result ￼. In that case, we perform the evaluation normally (and we might choose to update the cache with the new argument/result as a separate entry, or simply ignore caching for this call).
	5.	In the extremely unlikely event of a true hash collision (distinct values with same hash), the equality check prevents a wrong result. The only cost is that we did an O(N) comparison plus the failed lookup.

By storing the original argument alongside the result in the cache (rather than just the hash), we make this collision check possible ￼. A simple structure could be:

struct MemoEntry {
    Value* arg;
    Value* result;
    size_t argsHash;
};

Here arg and result are pointers to the original argument and result values (which we will ensure remain valid by rooting them, discussed later). We also keep argsHash for quick checks or rehashing. On a cache hit, we verify with equalValueLazy(*args[0], *entry.arg) ￼. Only if this returns true do we use entry.result.

This mechanism ensures correctness: even if our hashing was imperfect or unlucky, we won’t return an incorrect result because structural equality (no forcing) must hold for the cache to hit. If any part of the structure differs or isn’t comparable due to laziness, we fall back to a safe path. This is analogous to how Haskell’s memoization libraries defend against collisions by using Stable Names (for hashing) plus actual equality checks on normal forms if needed ￼ ￼.

To illustrate, consider two different attrsets that hash to the same value (maybe due to a collision). When we compare them field-by-field, we’ll find the first differing field and report inequality, avoiding a bad cache hit. Or if one of them has a thunk that the other doesn’t, we immediately don’t consider them equal.

One more subtle point: what if the argument was partly lazy and we did allow caching (for instance, an optimistic strategy that hashes thunks by identity)? In that case, the equality check might encounter a thunk in the cached argument and an equivalent thunk in the new argument. Since we decided that if either is thunk we return false, the cache won’t be used. This is safe (no incorrect result), but it means our optimistic caching didn’t help in that scenario – effectively the same as not caching at all for thunky values. A more complex approach could try to say “these thunks are the same unevaluated computation” if they have the same (Env*, Expr*) and perhaps treat that as equal. But due to the GC reuse problem, it’s dangerous unless we have stable IDs for thunks. For now, the simplest correct choice is to not treat unevaluated thunks as equal, even if they look identical. They must actually be the same object (which if they were, they’d both be forced together anyway by Nix’s sharing).

GC Safety: Boehm Pointer Reuse and Stable Identifiers

Boehm GC and pointer reuse: Nix’s evaluator uses the Boehm-Demers-Weiser conservative garbage collector. Boehm is a non-moving GC (it doesn’t compact or relocate objects), which means pointers to values remain valid as long as the object is alive. However, once an object is collected, Boehm may reuse that freed memory for a new allocation. Thus, over time, the same memory address could correspond to different objects. This poses a big problem if our memoization key relies on raw pointer values (addresses). For example, a thunk’s address might be reused for an unrelated thunk later, and if we cached by pointer, we could falsely think we’ve seen this thunk before ￼. Actual incidents of this were observed, causing incorrect evaluation results when a cache returned a result for a completely different computation that happened to get the same pointer after GC ￼.

To ensure GC-safety in our memoization cache, we must eliminate or mitigate reliance on raw pointer identities that can be recycled. We touched on one approach: assigning unique IDs to objects on allocation ￼. This way, even if an address gets reused, the new object gets a different ID, so it won’t compare equal or hash equal to the old object’s ID. Implementing this might involve adding an id field to the Value struct or keeping an external table of pointer->ID mappings. The ID could be a 64-bit counter that increments for each new thunk/value created, which should never overflow in a single evaluation (and if it does, wrap-around can be treated as collision to handle).

Another approach seen in other runtimes is to use stable pointers/names. Haskell’s StableName API, for instance, creates a token for an object that can be compared for equality and hashed ￼. Under the hood, GHC ensures that token remains valid even if the object moves or is collected (the token is typically a pair of an address and an entry in a stable name table) ￼. In our context, since Boehm doesn’t move objects, a stable ID as described is similar to a stable name.

Maintaining object longevity: Another GC-related consideration is ensuring that objects in the cache don’t get collected while still in use. If we insert a {arg, result} pair in the cache and nothing else references those values, we must prevent GC from freeing them (otherwise the next lookup might be comparing a new argument to a freed cachedArg pointer!). There are a few strategies for this:
	•	GC roots (strong references): We can mark cached values as globally reachable. For example, Nix has a concept of RootValue or similar to protect a Value* from GC ￼. By wrapping the cached argument and result in a root, we ensure they stay alive until we remove them from the cache. Our cache entry structure could hold RootValue arg; RootValue result; rather than raw pointers ￼. The Boehm GC will see these as conservative roots (if allocated in the root set or registered via the Boehm API), keeping the actual objects around. This is the simplest way to avoid dangling pointers in the cache.
	•	The downside: this can lead to memory leaks or increased memory usage, since cached values won’t be freed even if they’re not needed again. If we memoize a lot of different calls, we might hold onto all their args and results.
	•	Weak references: A more sophisticated approach is to use weak pointers for cache keys (and/or values). A weak pointer is ignored by GC’s liveness tracing, so it doesn’t keep the object alive. If an object only has weak references pointing to it, it can be collected. In a cache, we could hold the argument as a weak reference. If the argument gets collected, we should also remove the cache entry, since we can no longer validate it properly (and it likely means that result won’t be needed again). Some implementations use weak keys to let caches automatically drop entries when keys are gone ￼.
	•	Boehm GC does support weak pointers in a limited form (via GC_malloc_atomic and finalizers or using an add-on library for weak maps). Another way is to manually check, e.g., keep a timestamp or generation number to detect stale entries.
	•	The stable-memo Haskell library is instructive: it uses stable names (IDs) for keys and weak pointers with finalizers to prune the cache when the key object is collected ￼. We can mimic this by, for instance, associating a finalizer with the arg Value that calls back and removes the entry from our cache table.
	•	Never reusing keys directly: We could avoid storing the raw Value* of the argument as the key altogether. Instead, store only an immutable representation (like the structural hash and perhaps the original AST if needed). But since we need equality checks, we’d still need the original value or something equivalent. Storing a copy of the argument could be an option (deep-clone the structure at cache insert time). That copy would then be fully owned by the cache and can be compared later. This avoids dealing with the original being GC’d, but copying could be extremely expensive and also complicated for lazy thunks.

In practice, a feasible plan is: root the cached values to guarantee safety, and optionally use weak references to cleanup and avoid unbounded growth.** For example, at insertion, wrap the argument and result in RootValue (so they don’t die) ￼, but also register a finalizer on the argument Value that will drop the root and remove the entry when GC is about to collect it. The finalizer would only run when the program itself has no other references to that value (except the weak one we hold separately). Implementing this requires careful coordination with Boehm’s API for finalizers (Boehm allows registering finalizers on objects). We have to ensure the finalizer sees the entry to remove.

Pointer identity in equality: We mentioned using pointer (or ID) equality for functions and for recognizing if two values are literally the same object. Here, Boehm’s non-moving nature is helpful: if two Value* are equal (point to the same address), they are the same live object (unless it was collected and coincidentally reallocated, which our ID scheme prevents confusion about). So we can use pointer equality as a quick path in equality checking (e.g., if a == b pointers, return true immediately). For thunks or values that we decided not to hash, we might also say if the exact same thunk object is passed again, we could consider that a hit. In fact, Nix already does sharing within an eval: if you refer to the same variable or literal in multiple places, it’s the same Value* pointer ￼ ￼. So one could imagine detecting that and skipping evaluation. But the more general memoization is about distinct instances that are structurally equal, so pointer equality alone is insufficient.

To summarize GC considerations:
	•	We avoid keys that depend on ephemeral addresses by using stable IDs or content-based keys.
	•	We keep cached objects alive to prevent dangling pointers (with the option of weak removal to avoid growth).
	•	We account for Boehm’s quirks, ensuring we clean up entries if needed to prevent pointer reuse issues.
	•	By storing the original arguments/results (or their protected references), we ensure that equality checks have valid data to work with on a cache hit.

This approach parallels what other runtimes do: for example, GHC’s stable names provide an ID that doesn’t get reused until the program ends ￼, and weak pointers with finalizers allow caches to not leak memory ￼. Some language runtimes even offer an ephemeron or weak hash table abstraction (key is weak but alive as long as key and value are both alive), which is ideal for memo caches. We might simulate a basic version: keep a global structure and when GC runs, scan for freed keys. But given Nix’s evaluation model (often short-lived processes for each eval), a simpler approach might be acceptable.

Lessons from Other Lazy Evaluators (Haskell, OCaml, Scheme)

Memoization in lazy functional languages is a well-studied problem. It’s instructive to see how some other systems handle caching, equality, and thunks:
	•	Haskell (GHC): Haskell is pure and lazy, and by default it only shares common expressions within a single evaluation (not across separate calls). The GHC runtime provides primitives to support memoization without forcing evaluation:
	•	Stable Names: GHC’s System.Mem.StableName API gives you a way to perform O(1) equality comparisons on objects by identity ￼. You can think of it as assigning a hidden unique ID to a thunk or value at runtime. If two stable names compare equal, they refer to the same object (guaranteed). If they don’t compare equal, the objects might still be equal in value, but at least they’re not the same instance ￼ ￼. Importantly, creating a stable name does not evaluate the thunk – it just tags the object. This is exactly what we want: an identity that doesn’t force evaluation.
	•	Weak Pointers: GHC also has Weak# primitives (used via System.Mem.Weak in Haskell) that allow the program to hold a reference to an object that doesn’t prevent it from being GC’d ￼. Along with weak pointers, GHC supports attaching finalizers that run when an object is collected. This is used in memoization to drop cache entries when keys are gone ￼.
	•	Memoization libraries: Libraries like stable-memo combine stable names and weak pointers to implement caches. They memoize by argument identity rather than deep equality ￼. Essentially, the first time a function is called on a particular argument (some heap object), they create a stable name for that argument and store the result in a hash table keyed by the stable name. Because stable names are only equal for the exact same object, this approach catches when the very same thunk or data structure is passed again. It won’t treat two separately constructed but equal structures as the same (unless they share the same backing object). This is a bit different from structural hashing, but it’s lazy-safe and often sufficient for things like graph algorithms where you want to avoid re-processing the same node object.
	•	The stable-memo implementation only evaluates keys to weak head normal form (WHNF) ￼ (ensuring it has a heap object for the argument) and uses weak references so that if the argument is later garbage, the entry is removed ￼. It even provides a mode where the results are held weakly too (so both key and value can be collected if not used again) ￼.
	•	Under the hood, as mentioned, it is based on the paper “Stretching the storage manager: weak pointers and stable names in Haskell” by Peyton Jones et al. ￼, which introduced these concepts to GHC’s runtime. The paper addresses how to implement these features efficiently in a garbage-collected language ￼.
	•	Implication for Nix: We can emulate stable names by unique IDs, and use Boehm’s facilities for weak pointers/finalizers to similarly avoid leaks. One caution is that stable-name-based memoization only catches repeated identical references, not all structurally-equal values. Nix’s goal is a bit broader: even if you load two separate copies of nixpkgs (so they are distinct in memory but logically the same store paths and values), we want to avoid double work ￼. That suggests we do need structural comparisons, not just pointer identity. But stable names could help for internal deduplication if the same thunk is passed around.
	•	Haskell also teaches us that pointer equality can be dangerous if it alters program logic. There’s a known issue that if you had a hypothetical pointer equality check in Haskell, it could distinguish values that are equal but not yet evaluated, which can change laziness/strictness properties (e.g., you could use pointer eq to short-circuit a computation that would otherwise diverge) ￼. By sticking to using it only for memoization (an optimization) and not for deciding pure results, we follow the safe pattern.
	•	OCaml: OCaml is a strict language (eager evaluation), so it doesn’t have the exact same laziness issues. Memoization in OCaml is typically done by hashing or comparing fully evaluated arguments:
	•	The standard library provides functors to build memoization tables, usually requiring the user to provide a hash and equality for the key type (or to use generic structural comparison if the type supports it). Since arguments in OCaml are evaluated before the function sees them, using them as keys is straightforward if they are immutable. For example, memoizing a function f : string -> int can use the string’s content as the key (OCaml’s strings are already hashable and comparable).
	•	OCaml polymorphic equality (= operator) will do deep comparisons of structures and it can handle cycles by not looping forever (it will detect a cycle and return false, or in some cases raise an exception) ￼. It will not force lazy values (of type Lazy.t); if you compare two lazy values, I believe it compares their internal pointer or state. Two Lazy.t are equal only if they are the same object (reference equality), or if both are already evaluated and their results are equal. This is analogous to our approach: we won’t force thunks, and two thunks in Nix would only compare equal if they are literally the same object (which our equality function could treat as a special case if desired).
	•	Although OCaml doesn’t need stable names for pure code, it has something called Oo.id for objects which gives a unique int per object (similar idea to stable ID). That’s more for object identity than memoization.
	•	Persistent data structures: OCaml (and Haskell, etc.) have persistent (immutable) data structures, like maps/sets, that sometimes cache their hash or size. For example, an immutable set might store its size so you don’t have to recompute it, or even a hash for the whole structure. Nix attrsets and lists are immutable once created (except for thunks inside them), so we could consider storing a precomputed hash in those structures at construction time. Clojure’s persistent collections do this: e.g., a Clojure vector will compute its hash code the first time asked and memoize it. If Nix did that, hashValueLazy could just read the memoized hash of a list or attrset if already computed. But computing it lazily still has to handle thunks carefully. It might not be computed at all if never needed.
	•	Memoization vs laziness trade-off: In OCaml, since evaluation is strict, adding memoization doesn’t risk evaluating things that wouldn’t be evaluated anyway. In Nix, we must be more careful. So OCaml’s memo techniques (like using a Hashtbl with structural keys) would need adjustment to avoid forcing lazy subparts.
	•	Scheme/Lisp: Most Scheme implementations are strict (Racket, Guile, etc., evaluate function args eagerly). Laziness is usually user-managed via explicitly creating thunks (using delay and force). Scheme’s equality predicates are interesting:
	•	eq? is basically pointer/reference equality (true only if two values are the exact same object). It’s very fast but not useful for numbers or for structural comparison.
	•	equal? does a deep structural comparison of lists, vectors, and strings (and can be extended to other types) ￼. To avoid infinite loops on cycles, many Scheme implementations have cycle detection in equal?. This is analogous to our equalValueLazy needing to handle cycles.
	•	Memoization in Scheme is often done by manually creating a table (e.g., using an association list or a hash table) and using equal? as the key comparator when appropriate. If the keys are numbers or strings, one might use eq? if interns or unique objects are used. For compound structures, equal? ensures proper structural matching. But if those structures contain promise (lazy) objects, equal? typically will force them if it tries to compare their content (depending on the implementation). So one must be cautious – perhaps treat promises as opaque in keys (like our approach).
	•	Hash-consing: In Lisp and ML culture, there’s a concept of hash-consing, which is making a canonical representation for structurally identical data. For example, you maintain a global table of cons cells; when you need a new cons, you first check if one with the same car and cdr already exists, and if so, reuse it. This way, any two equal lists are literally the same pointer (eq?). Some Scheme libraries or ML systems use this to save memory and speed up equality checks ￼. In our context, if we could hash-cons Nix values, then structural equality becomes pointer equality. But implementing hash-consing for arbitrary Nix values at runtime would require intercepting every value construction and doing a table lookup, which is expensive. It’s also complicated by lazy thunks (you’d have to decide if a thunk with the same code and env as another should be merged – probably not safe because they might produce different results if env differs or side effects). So full hash-consing is not suitable for Nix evaluation, though it’s a conceptual extreme of content-addressing.
	•	Weak maps: Many Scheme implementations provide weak hash tables as a library feature (e.g., in Guile or Racket you can make a hash table with weak keys or values). This is often used to implement memoization caches that don’t leak. Nix’s C++ environment can use Boehm’s capabilities similarly as discussed.

Key takeaways from other languages: Use identity-based shortcuts with GC support (stable names, unique IDs) to avoid full deep comparisons when possible, use weak references or similar to avoid retaining unnecessary data, and ensure that any equality/hashing does not violate laziness or purity. Haskell’s approach is closest to what we need because Haskell deals with laziness too – it basically says: don’t force, just use identity of thunks. But Haskell leaves it to the library/user to ensure that using identity is semantically okay (which in pure code it is, for optimization purposes). We in Nix can enforce that at the evaluator level.

Related Work: Content-Addressed Memoization and Graph Caching

Beyond specific languages, there are research ideas and systems that tackle content-addressable computation and graph reduction caching, which we can learn from:
	•	Content-Addressed Storage/Computation: The Nix store itself is content-addressed (store paths include a hash of the build inputs). Similarly, one could imagine making the evaluation content-addressed, i.e., identifying sub-expressions by a hash of their contents and reusing results. Some experimental build systems and VMs have explored this. For example, the concept of a “content-addressed function result” or CAS for computations: if you know a function f with input x produces output y, you store an entry keyed by (hash(f), hash(x)) = hash(y). In a pure setting, this is feasible.
	•	Nix Flakes actually introduced a coarse-grained eval cache: if a flake’s inputs (nix files, versions, etc.) haven’t changed, you can reuse a previous evaluation of that flake ￼ ￼. But that’s at the flake or top-level output granularity. We want finer granularity.
	•	Some academic systems have content-addressable memoization tables that persist across runs (like memoizing to disk). Nix could benefit from that eventually (imagine caching the evaluation of import <nixpkgs> globally so next time it’s instant if the commit hash is the same). The question here is more about in-memory sharing within one run, but designing it with content hashes could allow persisting the cache.
	•	If we had a perfect content hash for Nix values (one that fully reflects the value’s normal form without evaluating it fully), we could key the cache on that. This verges on solving the function equality problem, which is generally undecidable (we can’t hash an unknown function’s behavior easily without running it). However, for data values, cryptographic hashes of fully evaluated data are sometimes used as surrogates for the data itself (like in deduplication).
	•	There is research on holistic memoization in lazy languages, sometimes called “optimal reduction” or graph reduction that attempts to reuse identical subgraphs. The “full laziness” transformation in compilers tries to turn repeated identical expressions into shared expressions (so they only compute once) – e.g., turning f (expensive) + g (expensive) into let t = expensive in f t + g t. What we are doing is like a runtime version of full laziness, where we notice at runtime that expensive was computed before and reuse it. Content-addressing helps identify those cases even if not obvious from syntax.
	•	A recent line of work is on Incremental computation frameworks (like Adapton, self-adjusting computation, etc.) which create a dependency graph of computation such that if some input (sub-tree) is repeated or changes, only necessary recomputation happens ￼ ￼. These frameworks often assign hashes to nodes and use those to quickly check if a recomputation yields the same result. They sometimes use demand-driven hashing, meaning a hash for a result node is computed from the hashes of dependencies, and only updated when those dependencies change. If the hash remains the same, the output is assumed the same and computation can be skipped.
	•	In a Nix context, demand-driven hashing might mean: as we evaluate, we compute hashes for values on the fly. If a function’s output is requested and we can derive that the inputs hash to something we’ve seen, we skip actual eval and reuse cached output. But implementing this on the fly is complex – essentially maintaining a memo graph with invalidation. Nix eval isn’t interactive or incremental (it’s usually one-shot), so a simpler approach (cache pure function calls in a table) is more straightforward.
	•	Another research area is graph rewriting systems that memoize function applications. Some functional logic languages (like Mercury or some Prolog engines with tabling) automatically cache calls and results (called memoing or tabling in those contexts). They ensure termination and sharing by storing a table of calls (keyed by arguments) and results. The issues they face (ensuring correct equality, managing table size, dealing with variables/laziness) overlap with ours. One known technique in logic programming is variant checking – determining if a new call is a variant of a previous call (same up to variable renaming) and reusing the answer. For Nix, our “variant” check is structural equality of arguments.
	•	Hash collisions & cryptographic hashes: If one were extremely paranoid about hash collisions, one could use a cryptographic hash (SHA-256, etc.) of the structural representation as the key, instead of a machine size_t. This reduces collision probability to near-zero. It’s more expensive to compute, but maybe acceptable for big arguments that are expensive anyway. Alternatively, a two-tier hashing (like 64-bit for table lookup, plus if that matches, a 256-bit for final verification) could be used. For now, our approach of hash + equality is sufficient, but this is an option for future robustness.
	•	Content-addressed values: As hinted, we could modify Nix’s Value so that each value computes and stores a hash of its content at construction or upon first need. For instance, every attrset could carry a SHA-1 of its keys and values (with values hashed recursively). Thunks could potentially carry a hash of their code if we choose. This essentially creates a hash-consed system. If two values ever computed the same hash, one could even short-circuit and reuse one – but only after confirming equality to avoid collision issues. This might be more overhead than it’s worth, but it’s conceptually similar to how Git or Merkle trees avoid recomputing identical subtrees.
	•	Graph cache invalidation: One thing to note is that within one evaluation, we don’t have changes to inputs – we just have repeated identical calls. So we don’t need a fancy invalidation scheme; we just need detection of repeats. If Nix had a long-running evaluator where expressions can be evaluated, garbage collected, and new ones come in (like an interactive environment), then we’d also care about when to evict cache entries. In our case, eviction is mostly for memory management rather than correctness (except in the GC pointer reuse scenario, where eviction or avoidance is required for correctness).

In summary, research and related systems suggest using hashes as identifiers for computations and building a cache of results, which is exactly our plan. The tricky part is doing it lazily and safely, which we handle with structural hashing and equality. Approaches like stable naming, hash-consing, and incremental hashing all inform our strategy:
	•	We will assign stable IDs (akin to stable names) to avoid pointer reuse issues.
	•	We consider using cryptographic or strong hashes for content addressing if needed.
	•	We will include structural equality checks (like variant checks in tabling) to guarantee correctness.
	•	We might not implement full demand-driven update of hashes (since Nix doesn’t have long-lived incremental evaluation), but if needed, we could refine our hash if a thunk becomes evaluated during a function call (not addressed here, but an idea: compute a partial hash, and if during evaluation some thunks force others, one could update the cache key. This is complex and likely not necessary if we simply skip caching when thunks are present).

Implementation Strategy and Example (Putting It All Together)

Combining the above insights, here is a high-level design for implementing memoization in Nix’s C++ evaluator:
	1.	Identify Memoization Points: We target function calls (applications of lambdas) as the point to inject caching ￼. Specifically, in EvalState::callFunction, when a function value f of type tLambda is being applied to an argument, we will perform a cache lookup before evaluating the function body ￼. We do not memoize primitive operations (those are usually cheap or have side effects we shouldn’t skip) unless known pure and expensive. We also do not memoize when the function is something like a blackhole (in evaluation) or when arguments are not suitable (see next).
	•	The cache key will be based on the function’s identity and the argument’s structure. We can define struct CallMemoKey { ExprLambda* code; Env* env; size_t argsHash; }. Here, code and env together identify the closure (function value) ￼, and argsHash is the structural hash of the provided argument.
	•	We assume single-argument functions for simplicity (Nix functions technically take one argument, which can be an attrset for multiple params). If we extend to multiple arguments (currying), the key can include all argument hashes or a combined hash for the argument tuple.
	2.	Compute Argument Hash Lazily: When callFunction is invoked:
	•	If memoization is enabled (could be behind a flag or always on), call hashValueLazy(argValue). This returns a size_t argsHash.
	•	If argsHash == 0 (our sentinel for “contains thunks” or otherwise not hashable), then we do not attempt caching for this call ￼ ￼. We proceed with normal evaluation. Rationale: the argument isn’t fully evaluable structurally (it has lazy parts), so we skip caching to avoid forcing. (Optional optimization: we could still insert an entry with a special tag so that if the exact same thunk pointer is passed again, we might detect it. But that’s an advanced case – likely not worth it initially, as those thunks would be different objects on re-evaluation of the same expression in separate calls.)
	•	If argsHash != 0, we construct a CallMemoKey with the function’s code pointer, env pointer, and this hash.
	3.	Lookup in Concurrent Cache: We maintain a global or evaluator-scoped ConcurrentHashMap<CallMemoKey, MemoEntry> (using something like boost::concurrent_flat_map or a custom thread-safe structure, since Nix might evaluate in parallel in the future). The key equality for the map will consider code, env, and argsHash all must match. We then:
	•	Attempt to find an entry. If found, we then perform the equality check as described: if (equalValueLazy(currentArg, entry.arg)) { use entry.result } else { treat as miss } ￼.
	•	If the equality returns true, we have a cache hit. We can return the cached result immediately, skipping evaluating the function body. This result is a Value* that we had stored. Because we store it as a RootValue (or otherwise ensure it’s valid), it should still be a fully evaluated value ready to use. We might need to copy it or clone if we worry about one caller mutating something, but since Nix values are immutable (except thunks forcing themselves), returning the same pointer is fine. (All callers will see the same Value; this is actually beneficial for sharing.)
	•	If no entry was found, or if equality check failed, it’s a cache miss. In this case, we proceed to actually evaluate the function:
	•	Force the function (it’s a lambda, already done when we entered callFunction typically) ￼.
	•	Create a new environment with the argument bound (but note: we do not force the argument itself at this point, it remains lazy inside the function unless used) ￼.
	•	Evaluate the function body to get a result Value.
	•	After obtaining the result, we consider inserting it into the cache. We should only insert if our earlier argsHash was not the sentinel (meaning the argument had no problematic thunks when hashed). But what if the argument contained thunks that got forced during evaluation?
	•	Example: argument is a huge attrset of lazy values, and the function ends up forcing many of them. Initially, argsHash was 0 (so we didn’t cache). We won’t cache after either, so that opportunity is lost.
	•	Another example: argument had no thunks at top-level (non-thunk structure), we hashed it, but during evaluation the function forces some nested thunks inside it. Our hash might not reflect those now-forced values vs their previous lazy state. However, since we decided they were opaque at hash time, this call wasn’t skipped. Now that those thunks are forced, if we were to cache this result, on a subsequent call the argument might come in already forced (or not). If it’s not forced next time, we wouldn’t even use the cache (argsHash would again be 0). So caching it is moot in that scenario.
	•	Therefore, to keep it simple: only cache if argsHash was non-zero (no thunks encountered). That means the argument was already fully structurally evaluated (or was a shallow structure containing thunks that we intentionally ignored by sentinel). In practice, many Nix function args (like the system string in nixosSystem { system = "x86_64-linux"; ... }) are simple values and hashable. For complex ones like attrsets of config, currently we’d skip caching due to thunks. We might improve that later with advanced strategies.
	•	When inserting, we create a MemoEntry holding the original argument and result. For safety, we wrap them in a GC root (to keep them alive) ￼. We then insert into the concurrent map with the key.
	•	If another thread was racing and inserted the same key meanwhile (rare, but possible if parallel evaluation of same function), we should handle that (maybe discard duplicate or keep one – but if our equality is correct, both results should be the same semantically).
	•	If equality check failed on a found entry (hash collision scenario), we might choose to insert the new entry under the same hash (but then our key would collide – our key uses hash as part of identity). Notably, if code and env are same but argsHash is same while arguments differ, that means a collision in our hash function. Extremely unlikely for good hash, but possible. One way to handle this is to refine the key: include a secondary hash or a collision count. Simpler: we could treat it as “no caching for you” scenario or use a list of entries for that hash and linear search by equality. But given the rarity, simply not caching that call (or leaving the first entry only) is acceptable. The impact is just lost performance on that particular function for those arguments.
	4.	Memory Management and Cleanup: As discussed, we plan to root cached values to keep them valid. This means the cache can grow in memory. We should mitigate unbounded growth:
	•	We could limit the cache size or entries per function, evicting least-recently-used entries if needed. This complicates things (especially in a pure evaluator that typically runs quickly and then exits – might not be necessary).
	•	We could use weak keys: for example, do not root the argument. Instead, rely on the cached result likely holding enough of the data graph alive (in many cases the result will reference the argument or parts of it if it’s pure, but not always). If the argument is collected, we remove the entry. However, if the result still exists, maybe the argument was embedded in it… tricky.
	•	A conservative approach for a first implementation is to leave entries alive for the duration of the evaluation and clear the cache at the end of the top-level eval. Nix evaluations usually aren’t long-running processes (except maybe the Nix repl or daemon). If we evaluate two configurations in one run (like the flake example), the cache lives through both and then can be dropped.
	•	If we integrate with Nix’s existing GC, we could periodically purge entries whose arg is no longer reachable except by the cache itself (which we can detect if we use weak references or finalizers). This is an optimization.
	5.	Ensure Laziness and Correctness: Our design inherently avoids forcing thunks just to hash or compare. We either skip caching or treat not-equal if things are not fully evaluated. This means we might lose some caching opportunities (we are sound but not complete in terms of catching all identical calls). We accept that trade-off for correctness.
	•	We must be careful that using the cached result doesn’t inadvertently cause something to happen that wouldn’t have happened. For pure results, returning a cached value or a freshly computed one is the same from a functional perspective. But consider side effects: e.g., a Nix function that calls trace (a debugging print) will print on the first call. If the second call hits the cache, it will not print again, changing the observable behavior (no duplicate trace) ￼. This is usually fine (it makes the function idempotent in terms of side effects on repeated calls, which many consider acceptable for a cache) – but it is a change to be aware of. Since Nix expressions are pure and don’t have I/O, the main side effects are things like logging or non-deterministic builtins (which are rare/unsafe in pure eval). The design in the Nix issue tracker suggested maybe running in a “pure mode” to avoid caching impure results ￼. Our assumption here is we mainly cache pure computations (like large attrsets).
	•	Currying: If a function returns another function (common in Nix), our caching of the outer call will cause the same inner function object to be returned on subsequent calls, rather than a fresh one each time ￼. Since Nix functions are pure, returning the same closure vs a new identical closure shouldn’t matter for the final outcome. It can matter for memory (we don’t allocate a new closure second time) and for debug (if one is printing addresses, but that’s not usual). It also means any trace inside the outer function that happens before returning the inner function would only run once on the first call ￼. This is analogous to how Haskell’s unsafePerformIO with memoization would behave. We deem this acceptable as a consequence of memoization (and document it).
	•	We should also lock or use atomic operations to avoid double-computation: if two threads try to evaluate the same function with same arg at the same time, ideally one should do it and the other wait and then use the result. Our design currently doesn’t incorporate that (it would just let both compute and then both might insert; the second insertion might find an entry already present after computing). A more advanced design could have an entry in a “computing” state (e.g., insert a dummy before computing) to make others wait. This is an optimization for parallelism and preventing duplicated work in parallel; single-threaded Nix doesn’t need it.

Pseudo-code snippet for callFunction with memoization:

Value * EvalState::callFunction(Value & funVal, Value & argVal) {
    // Ensure funVal is evaluated to a function
    forceValue(funVal);
    if (!funVal.isLambda()) {
       throw Error("Trying to call a non-function");
    }
    Lambda * fun = funVal.lambda();
    // Compute lazy hash of argument
    size_t h = hashValueLazy(&argVal);
    CallMemoKey key{ fun->code, fun->env, h };
    MemoEntry entry;
    if (h != 0) {
        auto it = cache.find(key);
        if (it != cache.end()) {
            entry = it->second;
            auto eq = equalValueLazy(&argVal, entry.arg.get());
            if (eq && *eq) {
                // Cache hit: return cached result directly
                return entry.result.get();
            }
            // If eq is false or indeterminate, proceed to compute (collision or incomparability)
        }
    }
    // Not cached, actually call function
    Value * result = evalLambda(fun->code, fun->env, argVal);
    if (h != 0) {
        // Insert into cache
        entry.arg = RootValue(&argVal);
        entry.result = RootValue(result);
        entry.argsHash = h;
        cache.insert({ key, entry });
    }
    return result;
}

(This is a sketch; real code would need to handle more, like multi-argument via attrset destructuring.)

In this pseudo-code, RootValue is assumed to be a smart pointer that roots the value for GC (wrapping Value*). We check h != 0 to ensure we only cache hashable arguments ￼. On a hit, we verify equality ￼. If the equality function returns a tri-state (std::optional<bool> in our design, where “nullopt” means “had thunk, can’t decide” ￼), we also treat that as a miss. Then we do normal evaluation by evalLambda (which would create the new environment and evaluate the body). After getting result, we insert into the cache with the key and a MemoEntry holding rooted pointers to the arg and result.

For thread safety, the cache find/insert would need locking. We could use a concurrent map that handles it internally. If using a plain unordered_map, we’d wrap these in a mutex.

Example scenario: Consider the earlier example of two NixOS configurations using the same nixpkgs and system. Without memoization, each import nixpkgs { system = "..."; ...} evaluates a huge set of packages independently, doubling work ￼. With the above system:
	•	The first call import nixpkgs { ... } (which is a function call under the hood to a lambda in nixpkgs/lib.nixosSystem perhaps) will compute a hash of the argument attrset (likely 0 because that attrset has many thunks for each module, etc., so we skip caching for that high-level call). However, within that call, it might call lib.systems.elaborate system and other functions many times. If those inner functions are memoized and receive simple hashable args (like the string "x86_64-linux"), they will cache and save work on subsequent calls ￼.
	•	Now, the second configuration calls the same nixosSystem function with a similar attrset. If by chance the attrset was fully realized (unlikely, usually it contains closures for modules), we’d hash and possibly hit. More realistically, we might not catch it at the top level (because of thunks). But inside the evaluation of the second, when it calls into common library functions (with identical inputs as in the first evaluation), those will hit the cache. For instance, the large package set inside import nixpkgs might itself be produced by a fixed-point computation that could be memoized.
	•	Over time, as we refine this, more of the structure could be hashed (for example, if we implement optimistic hashing for thunks by ID + equality check, we might catch that the localSystem attrset passed into nixpkgs is the same in both calls and reuse the entire package set result).

The outcome is that repeated evaluations share substantial parts of their evaluation graph, trading a small overhead in hashing and checking for a large gain in not re-evaluating big expressions. Early experiments (as hinted in the provided docs) showed significant speedups when this works, e.g., nearly halving evaluation time for two identical configurations, with only minor overhead when values weren’t cacheable (the hashing overhead was negligible compared to recomputation) ￼.

Conclusion and Key Points

Implementing memoization in Nix’s evaluator is feasible by borrowing techniques from lazy functional languages and carefully addressing Nix-specific concerns:
	•	Structural Hashing: We compute hashes of arguments based on their structure and content, not their identity, and crucially we never force thunks to do so ￼ ￼. Thunks are either hashed by a safe surrogate (like an ID) or cause caching to be skipped. The hash covers all semantically relevant parts of values (including string contexts and recursive set structure) ￼ ￼.
	•	Collision Safety: We always perform a lazy structural equality check on a cache hit to confirm the argument truly matches the cached one before reusing a result ￼ ￼. This defends against hash collisions or any hash ambiguities. The equality is also non-forcing and handles cycles.
	•	GC Integration: We design the cache to be GC-safe by avoiding raw pointer keys that could be reused ￼. Using unique allocation IDs or stable pointers ensures that different objects won’t collide simply due to address recycling ￼ ￼. Additionally, cached values are kept alive (rooted) to ensure validity, and we consider using weak references to allow cache entries to expire when no longer needed ￼.
	•	Laziness Preservation: By skipping caching for arguments that aren’t fully evaluated (or treating unknown parts as unequal), we avoid accidentally evaluating things earlier than the program would normally require ￼. The memoization is semantically invisible except for performance (and minor side-effects like deduplicated trace outputs).
	•	Borrowed Wisdom: We cite Haskell’s use of stable names and weak pointers as a proven method for memoization in a lazy setting ￼ ￼. We also acknowledge techniques like hash-consing and content-addressing from functional programming research as inspiration for structural hashing and unique identifiers ￼.
	•	Persistent/Pure Results: Because Nix expressions are pure, caching is sound – the same function input will yield the same output. We avoid caching functions with obvious impurity or non-determinism (those should be treated carefully or not cached at all, similar to how the proposed Nix primop approach restricts to pure evaluations ￼).

By following this strategy, an experienced Nix developer can implement memoization within the C++ evaluator that significantly reduces duplicate work on identical evaluations, while maintaining the correct lazy semantics of Nix. The result is that large evaluations (like multiple NixOS systems or derivations sharing common inputs) can run faster and use less memory (due to shared sub-results) ￼, without requiring users to refactor their Nix code for explicit sharing. This is a complex feature touching the evaluator, garbage collector, and runtime representation, but its design can lean on existing techniques from functional language runtimes to ensure it is robust and effective.
